{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pip install arabert"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_zxlSB0dwHCB"},"source":["## Arabert\n","\n","This code will fine-tune the BERT model on the training data, and then test the model on the test data, printing the accuracy of the predictions. In this example, the dataframe is expected to have two columns, one for the text (named \"text\") and one for the labels (named \"hard_label\"). The labels are expected to be in boolean format, 0 or 1.\n","You may need to import tokenizer from transformers library before using it.\n","Please also note that this is a simplified example, you may want to consider adding other metrics like F1-score, precision, recall and so on, or using K-fold cross validation to get a more robust evaluation of your model."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7768,"status":"ok","timestamp":1674470472076,"user":{"displayName":"Giulia Rizzi","userId":"13349830956359138572"},"user_tz":-60},"id":"jDkxeN1qwLYA"},"outputs":[],"source":["import pandas as pd\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import BertTokenizer\n","import torch"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1674470456285,"user":{"displayName":"Giulia Rizzi","userId":"13349830956359138572"},"user_tz":-60},"id":"eLfw9IFGFBIE"},"outputs":[],"source":["path = r'G:/Il mio Drive/SemEval_Task11/LearningWithDisagreements/Data/data_practicephase_cleardev/data_practicephase_cleardev/'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1674470472077,"user":{"displayName":"Giulia Rizzi","userId":"13349830956359138572"},"user_tz":-60},"id":"6KqOODcoyy_v"},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":178,"status":"ok","timestamp":1674470474360,"user":{"displayName":"Giulia Rizzi","userId":"13349830956359138572"},"user_tz":-60},"id":"9mq1ysz2FGCp"},"outputs":[],"source":["# datasets paths\n","armis_train = path + 'ArMIS_dataset/ArMIS_train.json'\n","armis_dev = path + 'ArMIS_dataset/ArMIS_dev.json'\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":194,"status":"ok","timestamp":1674470558962,"user":{"displayName":"Giulia Rizzi","userId":"13349830956359138572"},"user_tz":-60},"id":"JP2tAOvpFU-9"},"outputs":[],"source":["# import and concat train datasets\n","df = pd.read_json(armis_train, orient='index')\n","df = df[['text', 'hard_label']]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["df_dev = pd.read_json(armis_dev, orient='index')\n","df_dev = df_dev[['text', 'hard_label']]"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>hard_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>النسويه يعني نصير رجل قولتك وبعدين اذا الوحده ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>هراء النسويات عمان والمتستر بتضخيم حالات العنف...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>بسبب انتشار الفكر النسوي القد ودعوة النسويات ا...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>وقال تعالى الن الل احا الل تأملي هذة الأيه</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>يعني السعوديات صايعات يتنقلو واحد للثاني</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>653</th>\n","      <td>رأيج محترم جدا ومنطقي وطرحج معقول اغلب اكدر اك...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>654</th>\n","      <td>تتع مئات الآلاف الفتيات والنساء يوميا جميع أنح...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>655</th>\n","      <td>وغيرها الداخل والخارج سافرات ومتعديات حدود الد...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>656</th>\n","      <td>والله ياصديقي انا اشفق عليها يسعفها عقلها ولم ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>657</th>\n","      <td>ينقص دينها لحديث معناه النساء ناقصات عقل ودين ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>657 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                  text  hard_label\n","1    النسويه يعني نصير رجل قولتك وبعدين اذا الوحده ...           0\n","2    هراء النسويات عمان والمتستر بتضخيم حالات العنف...           1\n","3    بسبب انتشار الفكر النسوي القد ودعوة النسويات ا...           1\n","4           وقال تعالى الن الل احا الل تأملي هذة الأيه           0\n","5             يعني السعوديات صايعات يتنقلو واحد للثاني           0\n","..                                                 ...         ...\n","653  رأيج محترم جدا ومنطقي وطرحج معقول اغلب اكدر اك...           1\n","654  تتع مئات الآلاف الفتيات والنساء يوميا جميع أنح...           0\n","655  وغيرها الداخل والخارج سافرات ومتعديات حدود الد...           1\n","656  والله ياصديقي انا اشفق عليها يسعفها عقلها ولم ...           1\n","657  ينقص دينها لحديث معناه النساء ناقصات عقل ودين ...           0\n","\n","[657 rows x 2 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":167,"status":"ok","timestamp":1674471123494,"user":{"displayName":"Giulia Rizzi","userId":"13349830956359138572"},"user_tz":-60},"id":"I-mJdwkSwf8f"},"outputs":[],"source":["# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(torch. __version__)\n","#python -m pip install torch==1.7.0 -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from arabert import ArabertPreprocessor\n","from arabert.aragpt2.grover.modeling_gpt2 import GPT2LMHeadModel"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["# select a model from the huggingface modelhub https://huggingface.co/models?language=ar\n","model_name = 'aubmindlab/bert-base-arabertv02-twitter' # we are going to use the twitter AraBERT since it has emojis and dialects\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["#Create and apply preprocessing using the AraBERT processor\n","arabic_prep = ArabertPreprocessor(model_name)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>hard_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>النسويه يعني نصير رجل قولتك وبعدين اذا الوحده ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>هراء النسويات عمان والمتستر بتضخيم حالات العنف...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>بسبب انتشار الفكر النسوي القد ودعوة النسويات ا...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>وقال تعالى الن الل احا الل تأملي هذة الأيه</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>يعني السعوديات صايعات يتنقلو واحد للثاني</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>653</th>\n","      <td>رأيج محترم جدا ومنطقي وطرحج معقول اغلب اكدر اك...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>654</th>\n","      <td>تتع مئات الآلاف الفتيات والنساء يوميا جميع أنح...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>655</th>\n","      <td>وغيرها الداخل والخارج سافرات ومتعديات حدود الد...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>656</th>\n","      <td>والله ياصديقي انا اشفق عليها يسعفها عقلها ولم ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>657</th>\n","      <td>ينقص دينها لحديث معناه النساء ناقصات عقل ودين ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>657 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                  text  hard_label\n","1    النسويه يعني نصير رجل قولتك وبعدين اذا الوحده ...           0\n","2    هراء النسويات عمان والمتستر بتضخيم حالات العنف...           1\n","3    بسبب انتشار الفكر النسوي القد ودعوة النسويات ا...           1\n","4           وقال تعالى الن الل احا الل تأملي هذة الأيه           0\n","5             يعني السعوديات صايعات يتنقلو واحد للثاني           0\n","..                                                 ...         ...\n","653  رأيج محترم جدا ومنطقي وطرحج معقول اغلب اكدر اك...           1\n","654  تتع مئات الآلاف الفتيات والنساء يوميا جميع أنح...           0\n","655  وغيرها الداخل والخارج سافرات ومتعديات حدود الد...           1\n","656  والله ياصديقي انا اشفق عليها يسعفها عقلها ولم ...           1\n","657  ينقص دينها لحديث معناه النساء ناقصات عقل ودين ...           0\n","\n","[657 rows x 2 columns]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df['text'] = df['text'].apply(lambda x: arabic_prep.preprocess(x))\n","df"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["from transformers import (AutoConfig, AutoModelForSequenceClassification,\n","                          AutoTokenizer, BertTokenizer, Trainer,\n","                          TrainingArguments)"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":456,"status":"ok","timestamp":1674471139217,"user":{"displayName":"Giulia Rizzi","userId":"13349830956359138572"},"user_tz":-60},"id":"6YabWrbMw6A_"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Helper function for formatting inputs\n","def format_inputs(text, labels):\n","    input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n","    attention_mask = torch.tensor([[1]*len(input_ids[0])])\n","    labels = torch.tensor([labels])\n","    return input_ids, attention_mask, labels"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02-twitter were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02-twitter and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=2)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\giuli\\anaconda3\\envs\\Semeval23\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# Define the optimizer and schedule (linear warmup and decay)\n","optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fine-tune the model\n","for epoch in tqdm(range(20)):\n","    model.train()\n","    train_loss = 0\n","    for i, row in df.iterrows():\n","        input_ids, attention_mask, labels = format_inputs(row['text'], row['hard_label'])\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs[0]\n","        train_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","    print(f'Epoch {epoch+1}, Loss: {train_loss/len(df)}')"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["torch.save(model, './model/arabert/fine_tuned_arabert.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cd ../"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["model=torch.load('./model/arabert/fine_tuned_arabert.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtX8HHe8xDhQ"},"outputs":[],"source":["# Test the model\n","model.eval()\n","test_loss = 0\n","for i, row in df_tot.iterrows():\n","    input_ids, attention_mask, labels = format_inputs(row['text'], row['hard_label'])\n","    input_ids = input_ids.to(device)\n","    attention_mask = attention_mask.to(device)\n","    labels = labels.to(device)\n","\n","    outputs = model(input_ids, attention_mask=attention_mask)\n","    logits = outputs[0]\n","    test_loss += loss"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"xMqo1FgJxT1u"},"outputs":[],"source":["# Helper function for getting predictions\n","def get_predictions(model, dataframe):\n","    predictions = []\n","    model.eval()\n","    for i, row in dataframe.iterrows():\n","        input_ids, attention_mask, _ = format_inputs(row['text'], row['hard_label'])\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        \n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs[0]\n","        pred = logits.argmax(dim=1).item()\n","        predictions.append(pred)\n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# SE SU CPU\n","model = model.to(device)"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"UNKNOaF6xV49"},"outputs":[],"source":["# Get predictions for the test set\n","test_predictions = get_predictions(model, df_dev)"]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":167,"status":"ok","timestamp":1674463601351,"user":{"displayName":"Giulia Rizzi","userId":"13349830956359138572"},"user_tz":-60},"id":"DEzHuouoxXuF","outputId":"cbb410cd-731e-4e55-c977-618627364a43"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7659574468085106\n"]}],"source":["# Compare predictions to true labels\n","df_dev['predictions'] = test_predictions\n","correct_predictions = df_dev[df_dev['hard_label'] == df_dev['predictions']]\n","accuracy = len(correct_predictions) / len(df_dev)\n","print(f'Accuracy: {accuracy}')"]},{"cell_type":"markdown","metadata":{"id":"IOyFgPefyJpJ"},"source":["### Integrated Gradient\n","\n","This code will use the integrated gradients method to calculate the attribution of each token in the input text to the final prediction of the model. The result is a list of values for each token, showing how much each token contributes to the final prediction.\n","You can also use other attribution methods such as ShapleyValue, Saliency or Deconvolution that are also available in captum library.\n","Please also note that this is a simplified example, you may want to consider adding other modifications such as running the explanation for multiple inputs to get a better idea of the model's behavior."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"xVWOgmJDyDh3"},"outputs":[],"source":["from captum.attr import IntegratedGradients"]},{"cell_type":"markdown","metadata":{"id":"EQMQYCc2C36K"},"source":["https://github.com/pytorch/captum/issues/150"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"tT0kstNh4Da-"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, BertConfig\n","\n","from captum.attr import IntegratedGradients\n","from captum.attr import InterpretableEmbeddingBase, TokenReferenceBase\n","from captum.attr import visualization\n","from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n","\n","\n","# We need to split forward pass into two part: \n","# 1) embeddings computation\n","# 2) classification\n","\n","def compute_bert_outputs(model_bert, embedding_output, attention_mask=None, head_mask=None):\n","    if attention_mask is None:\n","        attention_mask = torch.ones(embedding_output.shape[0], embedding_output.shape[1]).to(embedding_output)\n","\n","    extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n","\n","    extended_attention_mask = extended_attention_mask.to(dtype=next(model_bert.parameters()).dtype) # fp16 compatibility\n","    extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","    if head_mask is not None:\n","        if head_mask.dim() == 1:\n","            head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n","            head_mask = head_mask.expand(model_bert.config.num_hidden_layers, -1, -1, -1, -1)\n","        elif head_mask.dim() == 2:\n","            head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n","        head_mask = head_mask.to(dtype=next(model_bert.parameters()).dtype) # switch to fload if need + fp16 compatibility\n","    else:\n","        head_mask = [None] * model_bert.config.num_hidden_layers\n","\n","    encoder_outputs = model_bert.encoder(embedding_output,\n","                                         extended_attention_mask,\n","                                         head_mask=head_mask)\n","    sequence_output = encoder_outputs[0]\n","    pooled_output = model_bert.pooler(sequence_output)\n","    outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n","    return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)    "]},{"cell_type":"code","execution_count":30,"metadata":{"id":"P3WBEMwn4GGP"},"outputs":[],"source":["class BertModelWrapper(nn.Module):\n","    \n","    def __init__(self, model):\n","        super(BertModelWrapper, self).__init__()\n","        self.model = model\n","        \n","    def forward(self, embeddings):        \n","        outputs = compute_bert_outputs(self.model.bert, embeddings)\n","        pooled_output = outputs[1]\n","        pooled_output = self.model.dropout(pooled_output)\n","        logits = self.model.classifier(pooled_output)\n","        return torch.softmax(logits, dim=1)[:, 1].unsqueeze(1)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"h60sdmfP4Nmh"},"outputs":[],"source":["bert_model_wrapper = BertModelWrapper(model)\n","ig = IntegratedGradients(bert_model_wrapper)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"5VWutZyH4SMc"},"outputs":[],"source":["# accumalate couple samples in this array for visualization purposes\n","vis_data_records_ig = []"]},{"cell_type":"markdown","metadata":{"id":"Qy1Zs6BUYfjK"},"source":["https://github.com/pytorch/captum/blob/master/captum/attr/_utils/visualization.py"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"cgNTv3Yx3KBj"},"outputs":[],"source":["def interpret_sentence(model_wrapper, sentence, label=1):\n","\n","    model_wrapper.eval()\n","    model_wrapper.zero_grad()\n","    \n","    print(torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)]))\n","    input_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)]).to(device)\n","    \n","    input_embedding = model_wrapper.model.bert.embeddings(input_ids)\n","    \n","    # predict\n","    pred = model_wrapper(input_embedding).item()\n","    pred_ind = round(pred)\n","\n","    # compute attributions and approximation delta using integrated gradients\n","    attributions_ig, delta = ig.attribute(input_embedding, n_steps=100, return_convergence_delta=True)\n","\n","    print('pred: ', pred_ind, '(', '%.2f' % pred, ')', ', delta: ', abs(delta))\n","    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().data.numpy().tolist())    \n","    \n","    #add_attributions_to_visualizer(attributions_ig, tokens, pred, pred_ind, label, delta, vis_data_records_ig)\n","    return attributions_ig, tokens, pred, pred_ind, label, delta, vis_data_records_ig\n","    \n","    \n","def add_attributions_to_visualizer(attributions, tokens, pred, pred_ind, label, delta, vis_data_records):\n","    attributions = attributions.sum(dim=2).squeeze(0)\n","    attributions = attributions / torch.norm(attributions)\n","    attributions = attributions.detach().cpu().data.numpy()\n","    \n","    # storing couple samples in an array for visualization purposes\n","    vis_data_records.append(visualization.VisualizationDataRecord(\n","                            attributions,\n","                            pred,\n","                            pred_ind,\n","                            label,\n","                            \"hard_label\",\n","                            attributions.sum(),       \n","                            tokens[:len(attributions)],\n","                            delta))    \n"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>hard_label</th>\n","      <th>predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>النسويات  الفسويات</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  text  hard_label  predictions\n","2   النسويات  الفسويات           1            1"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["df_dev[1:2]"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[    2,   487, 12167, 18905, 19484, 38055,  5014,  7461,  8987,   870,\n","          1962,  1523,  7849,  7632, 27738,  1316,  5805, 19345,  5645,  3530,\n","         10918, 34082,     3]])\n","pred:  0 ( 0.24 ) , delta:  tensor([0.0044], device='cuda:0', dtype=torch.float64)\n"]}],"source":["attributions_ig, tokens, pred, pred_ind, label, delta, vis_data_records_ig = interpret_sentence(bert_model_wrapper, df_dev.loc[1,'text'], label=1)\n","add_attributions_to_visualizer( attributions_ig, tokens, pred, pred_ind, label, delta, vis_data_records_ig)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/html":["<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.24)</b></text></td><td><text style=\"padding-right:2em\"><b>hard_label</b></text></td><td><text style=\"padding-right:2em\"><b>0.65</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> الي                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> يقرأ                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> اسئلة                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> المعلمات                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> يستغرب                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> انهم                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> يعملون                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> معنا                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> نفس                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> الوزارة                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> الكثير                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> القيادات                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> النسائية                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> متسل                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##طات                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> فعلا                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> عندهم                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> سوء                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> فهم                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> وتطبيق                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> للأنظمة                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (0.24)</b></text></td><td><text style=\"padding-right:2em\"><b>hard_label</b></text></td><td><text style=\"padding-right:2em\"><b>0.65</b></text></td><td><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> الي                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> يقرأ                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> اسئلة                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> المعلمات                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> يستغرب                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> انهم                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> يعملون                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> معنا                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> نفس                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> الوزارة                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> الكثير                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> القيادات                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> النسائية                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> متسل                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##طات                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> فعلا                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> عندهم                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> سوء                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> فهم                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> وتطبيق                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> للأنظمة                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"],"text/plain":["<IPython.core.display.HTML object>"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["visualization.visualize_text(vis_data_records_ig)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def interpret_sentence(model_wrapper, sentence, label=1):\n","\n","    model_wrapper.eval()\n","    model_wrapper.zero_grad()\n","    \n","    input_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)]).to(device)\n","    \n","    input_embedding = model_wrapper.model.bert.embeddings(input_ids)\n","    \n","\n","    # compute attributions and approximation delta using integrated gradients\n","    attributions_ig, _ = ig.attribute(input_embedding, n_steps=100, return_convergence_delta=True)\n","    del model_wrapper\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    #print('pred: ', pred_ind, '(', '%.2f' % pred, ')', ', delta: ', abs(delta))\n","    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().data.numpy().tolist())    \n","    \n","    #add_attributions_to_visualizer(attributions_ig, tokens, pred, pred_ind, label, delta, vis_data_records_ig)\n","    return attributions_ig, tokens"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>hard_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>الي يقرأ اسئلة المعلمات يستغرب انهم يعملون معن...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>النسويات  الفسويات</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>سؤال للـ بسيط اتوقع تكلمنا التويتر دورات ومحاض...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>وبكذا أوكد استنتاجي بإن النسويات قسمين القسم ا...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>انت قلتي قلتي النسويه امرأة</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>137</th>\n","      <td>الرياضات القتالية حيلة نساء لمواجهة العنف المرأة</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>138</th>\n","      <td>احب النوع الفسويات لانها تجيب لحالها المسبه تنسحب</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>139</th>\n","      <td>اول شوف اخواتك وبعدين تكلم مكان صايعات ... عند...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>140</th>\n","      <td>ليييش ليش يحطون شيء بالنسويه خير مالقوا سبب قا...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>141</th>\n","      <td>ليه ممكن يكون بعن فها ليه غبية هاي أفكارها مثل...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>141 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                  text  hard_label\n","1    الي يقرأ اسئلة المعلمات يستغرب انهم يعملون معن...           0\n","2                                   النسويات  الفسويات           1\n","3    سؤال للـ بسيط اتوقع تكلمنا التويتر دورات ومحاض...           1\n","4    وبكذا أوكد استنتاجي بإن النسويات قسمين القسم ا...           1\n","5                          انت قلتي قلتي النسويه امرأة           0\n","..                                                 ...         ...\n","137   الرياضات القتالية حيلة نساء لمواجهة العنف المرأة           0\n","138  احب النوع الفسويات لانها تجيب لحالها المسبه تنسحب           1\n","139  اول شوف اخواتك وبعدين تكلم مكان صايعات ... عند...           1\n","140  ليييش ليش يحطون شيء بالنسويه خير مالقوا سبب قا...           0\n","141  ليه ممكن يكون بعن فها ليه غبية هاي أفكارها مثل...           1\n","\n","[141 rows x 2 columns]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["df_dev"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from tqdm import tqdm\n","import gc"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["141it [00:43,  3.22it/s]\n"]}],"source":["df_dev['attention']=0\n","df_dev['tokens']=''\n","for index, row in tqdm(df_dev.iterrows()):\n","  attributions_ig, tokens = interpret_sentence(bert_model_wrapper, sentence=df_dev.loc[index, 'text'], label=0)\n","  attributions_ig = attributions_ig.sum(dim=2).squeeze(0)\n","  attributions_ig = attributions_ig / torch.norm(attributions_ig)\n","  attributions_ig = attributions_ig.detach().cpu().data.numpy()\n","  df_dev.loc[index, 'attention'] = str(attributions_ig)\n","  df_dev.loc[index, 'tokens'] = str(tokens)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>hard_label</th>\n","      <th>attention</th>\n","      <th>tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>الي يقرأ اسئلة المعلمات يستغرب انهم يعملون معن...</td>\n","      <td>0</td>\n","      <td>[ 0.1534795  -0.030359   -0.09889226 -0.190681...</td>\n","      <td>['[CLS]', 'الي', 'يقرأ', 'اسئلة', 'المعلمات', ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>النسويات  الفسويات</td>\n","      <td>1</td>\n","      <td>[ 0.26387608  0.04718343  0.01067558  0.654362...</td>\n","      <td>['[CLS]', 'النسوي', '##ات', 'الفس', '##ويات', ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>سؤال للـ بسيط اتوقع تكلمنا التويتر دورات ومحاض...</td>\n","      <td>1</td>\n","      <td>[ 9.69040651e-01 -4.62940413e-02  7.14545812e-...</td>\n","      <td>['[CLS]', 'سؤال', '[UNK]', 'بسيط', 'اتوقع', 'ت...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>وبكذا أوكد استنتاجي بإن النسويات قسمين القسم ا...</td>\n","      <td>1</td>\n","      <td>[ 0.07688863 -0.10141717  0.11480371 -0.484018...</td>\n","      <td>['[CLS]', 'وبك', '##ذا', 'أوك', '##د', 'استنتا...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>انت قلتي قلتي النسويه امرأة</td>\n","      <td>0</td>\n","      <td>[ 0.21483734  0.37937387  0.29936994 -0.129498...</td>\n","      <td>['[CLS]', 'انت', 'قلت', '##ي', 'قلت', '##ي', '...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>137</th>\n","      <td>الرياضات القتالية حيلة نساء لمواجهة العنف المرأة</td>\n","      <td>0</td>\n","      <td>[ 0.94781497  0.04148038  0.02542426  0.011141...</td>\n","      <td>['[CLS]', 'الرياضات', 'القتالية', 'حيلة', 'نسا...</td>\n","    </tr>\n","    <tr>\n","      <th>138</th>\n","      <td>احب النوع الفسويات لانها تجيب لحالها المسبه تنسحب</td>\n","      <td>1</td>\n","      <td>[-0.05101407 -0.13024441 -0.10013564  0.634415...</td>\n","      <td>['[CLS]', 'احب', 'النوع', 'الفس', '##ويات', 'ل...</td>\n","    </tr>\n","    <tr>\n","      <th>139</th>\n","      <td>اول شوف اخواتك وبعدين تكلم مكان صايعات ... عند...</td>\n","      <td>1</td>\n","      <td>[-0.17444202  0.00055693  0.26633095 -0.397172...</td>\n","      <td>['[CLS]', 'اول', 'شوف', 'اخواتك', 'وبعدين', 'ت...</td>\n","    </tr>\n","    <tr>\n","      <th>140</th>\n","      <td>ليييش ليش يحطون شيء بالنسويه خير مالقوا سبب قا...</td>\n","      <td>0</td>\n","      <td>[ 0.23175431 -0.14728426 -0.31090029 -0.038534...</td>\n","      <td>['[CLS]', 'لي', '##يي', '##ش', 'ليش', 'يحطو', ...</td>\n","    </tr>\n","    <tr>\n","      <th>141</th>\n","      <td>ليه ممكن يكون بعن فها ليه غبية هاي أفكارها مثل...</td>\n","      <td>1</td>\n","      <td>[-0.58061939 -0.16646568  0.01822751 -0.138967...</td>\n","      <td>['[CLS]', 'ليه', 'ممكن', 'يكون', 'بع', '##ن', ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>141 rows × 4 columns</p>\n","</div>"],"text/plain":["                                                  text  hard_label  \\\n","1    الي يقرأ اسئلة المعلمات يستغرب انهم يعملون معن...           0   \n","2                                   النسويات  الفسويات           1   \n","3    سؤال للـ بسيط اتوقع تكلمنا التويتر دورات ومحاض...           1   \n","4    وبكذا أوكد استنتاجي بإن النسويات قسمين القسم ا...           1   \n","5                          انت قلتي قلتي النسويه امرأة           0   \n","..                                                 ...         ...   \n","137   الرياضات القتالية حيلة نساء لمواجهة العنف المرأة           0   \n","138  احب النوع الفسويات لانها تجيب لحالها المسبه تنسحب           1   \n","139  اول شوف اخواتك وبعدين تكلم مكان صايعات ... عند...           1   \n","140  ليييش ليش يحطون شيء بالنسويه خير مالقوا سبب قا...           0   \n","141  ليه ممكن يكون بعن فها ليه غبية هاي أفكارها مثل...           1   \n","\n","                                             attention  \\\n","1    [ 0.1534795  -0.030359   -0.09889226 -0.190681...   \n","2    [ 0.26387608  0.04718343  0.01067558  0.654362...   \n","3    [ 9.69040651e-01 -4.62940413e-02  7.14545812e-...   \n","4    [ 0.07688863 -0.10141717  0.11480371 -0.484018...   \n","5    [ 0.21483734  0.37937387  0.29936994 -0.129498...   \n","..                                                 ...   \n","137  [ 0.94781497  0.04148038  0.02542426  0.011141...   \n","138  [-0.05101407 -0.13024441 -0.10013564  0.634415...   \n","139  [-0.17444202  0.00055693  0.26633095 -0.397172...   \n","140  [ 0.23175431 -0.14728426 -0.31090029 -0.038534...   \n","141  [-0.58061939 -0.16646568  0.01822751 -0.138967...   \n","\n","                                                tokens  \n","1    ['[CLS]', 'الي', 'يقرأ', 'اسئلة', 'المعلمات', ...  \n","2    ['[CLS]', 'النسوي', '##ات', 'الفس', '##ويات', ...  \n","3    ['[CLS]', 'سؤال', '[UNK]', 'بسيط', 'اتوقع', 'ت...  \n","4    ['[CLS]', 'وبك', '##ذا', 'أوك', '##د', 'استنتا...  \n","5    ['[CLS]', 'انت', 'قلت', '##ي', 'قلت', '##ي', '...  \n","..                                                 ...  \n","137  ['[CLS]', 'الرياضات', 'القتالية', 'حيلة', 'نسا...  \n","138  ['[CLS]', 'احب', 'النوع', 'الفس', '##ويات', 'ل...  \n","139  ['[CLS]', 'اول', 'شوف', 'اخواتك', 'وبعدين', 'ت...  \n","140  ['[CLS]', 'لي', '##يي', '##ش', 'ليش', 'يحطو', ...  \n","141  ['[CLS]', 'ليه', 'ممكن', 'يكون', 'بع', '##ن', ...  \n","\n","[141 rows x 4 columns]"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["df_dev"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["df_dev.to_csv('./IG/arabic.csv', sep='\\t', index=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## IG on Test"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["path = r'G:\\Il mio Drive\\SemEval_Task11\\LearningWithDisagreements\\Data\\data_evaluation'"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# datasets paths\n","armis_test = path + '\\ArMIS_test.json'\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["df_test = pd.read_json(armis_test, orient='index')"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>annotation task</th>\n","      <th>number of annotations</th>\n","      <th>annotations</th>\n","      <th>annotators</th>\n","      <th>lang</th>\n","      <th>hard_label</th>\n","      <th>soft_label</th>\n","      <th>split</th>\n","      <th>other_info</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>اللسان السليط الص راخ الجراءة العنهجية والهمجي...</td>\n","      <td>misogyny and sexism detection in Arabic tweets</td>\n","      <td>3</td>\n","      <td></td>\n","      <td>Ann1,Ann2,Ann3</td>\n","      <td>ar</td>\n","      <td>NaN</td>\n","      <td>{'0': None, '1': None}</td>\n","      <td>test</td>\n","      <td>{'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>قناة الطهر أفضل قناة لمن أراد يحافظ أولاده تخد...</td>\n","      <td>misogyny and sexism detection in Arabic tweets</td>\n","      <td>3</td>\n","      <td></td>\n","      <td>Ann1,Ann2,Ann3</td>\n","      <td>ar</td>\n","      <td>NaN</td>\n","      <td>{'0': None, '1': None}</td>\n","      <td>test</td>\n","      <td>{'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>عنف يقال سيبويه أحرقت زوجته كتبه لأنه كان يشتغ...</td>\n","      <td>misogyny and sexism detection in Arabic tweets</td>\n","      <td>3</td>\n","      <td></td>\n","      <td>Ann1,Ann2,Ann3</td>\n","      <td>ar</td>\n","      <td>NaN</td>\n","      <td>{'0': None, '1': None}</td>\n","      <td>test</td>\n","      <td>{'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ليش لازم شيء نسويه عشان شخص لازم ترى عادي اقوم...</td>\n","      <td>misogyny and sexism detection in Arabic tweets</td>\n","      <td>3</td>\n","      <td></td>\n","      <td>Ann1,Ann2,Ann3</td>\n","      <td>ar</td>\n","      <td>NaN</td>\n","      <td>{'0': None, '1': None}</td>\n","      <td>test</td>\n","      <td>{'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>اساس قارنتوا طاعه الزوج بطاعة الرب عجزت افهم ا...</td>\n","      <td>misogyny and sexism detection in Arabic tweets</td>\n","      <td>3</td>\n","      <td></td>\n","      <td>Ann1,Ann2,Ann3</td>\n","      <td>ar</td>\n","      <td>NaN</td>\n","      <td>{'0': None, '1': None}</td>\n","      <td>test</td>\n","      <td>{'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>141</th>\n","      <td>تخيل مطوع قال ناقصات عقل دين</td>\n","      <td>misogyny and sexism detection in Arabic tweets</td>\n","      <td>3</td>\n","      <td></td>\n","      <td>Ann1,Ann2,Ann3</td>\n","      <td>ar</td>\n","      <td>NaN</td>\n","      <td>{'0': None, '1': None}</td>\n","      <td>test</td>\n","      <td>{'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...</td>\n","    </tr>\n","    <tr>\n","      <th>142</th>\n","      <td>تخيلي اقول الله ياخذ النساء اذا وحدة زعلت ردت ...</td>\n","      <td>misogyny and sexism detection in Arabic tweets</td>\n","      <td>3</td>\n","      <td></td>\n","      <td>Ann1,Ann2,Ann3</td>\n","      <td>ar</td>\n","      <td>NaN</td>\n","      <td>{'0': None, '1': None}</td>\n","      <td>test</td>\n","      <td>{'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...</td>\n","    </tr>\n","    <tr>\n","      <th>143</th>\n","      <td>الله يرحمه يغفرله يجعل مثواه الجنه يارب العالم...</td>\n","      <td>misogyny and sexism detection in Arabic tweets</td>\n","      <td>3</td>\n","      <td></td>\n","      <td>Ann1,Ann2,Ann3</td>\n","      <td>ar</td>\n","      <td>NaN</td>\n","      <td>{'0': None, '1': None}</td>\n","      <td>test</td>\n","      <td>{'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...</td>\n","    </tr>\n","    <tr>\n","      <th>144</th>\n","      <td>مافي الزمن النبي قال زمن الأزمنة يكون النساء أ...</td>\n","      <td>misogyny and sexism detection in Arabic tweets</td>\n","      <td>3</td>\n","      <td></td>\n","      <td>Ann1,Ann2,Ann3</td>\n","      <td>ar</td>\n","      <td>NaN</td>\n","      <td>{'0': None, '1': None}</td>\n","      <td>test</td>\n","      <td>{'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>احبكمم بنات وبالأخص النسويات</td>\n","      <td>misogyny and sexism detection in Arabic tweets</td>\n","      <td>3</td>\n","      <td></td>\n","      <td>Ann1,Ann2,Ann3</td>\n","      <td>ar</td>\n","      <td>NaN</td>\n","      <td>{'0': None, '1': None}</td>\n","      <td>test</td>\n","      <td>{'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>145 rows × 10 columns</p>\n","</div>"],"text/plain":["                                                  text  \\\n","1    اللسان السليط الص راخ الجراءة العنهجية والهمجي...   \n","2    قناة الطهر أفضل قناة لمن أراد يحافظ أولاده تخد...   \n","3    عنف يقال سيبويه أحرقت زوجته كتبه لأنه كان يشتغ...   \n","4    ليش لازم شيء نسويه عشان شخص لازم ترى عادي اقوم...   \n","5    اساس قارنتوا طاعه الزوج بطاعة الرب عجزت افهم ا...   \n","..                                                 ...   \n","141                       تخيل مطوع قال ناقصات عقل دين   \n","142  تخيلي اقول الله ياخذ النساء اذا وحدة زعلت ردت ...   \n","143  الله يرحمه يغفرله يجعل مثواه الجنه يارب العالم...   \n","144  مافي الزمن النبي قال زمن الأزمنة يكون النساء أ...   \n","145                       احبكمم بنات وبالأخص النسويات   \n","\n","                                    annotation task  number of annotations  \\\n","1    misogyny and sexism detection in Arabic tweets                      3   \n","2    misogyny and sexism detection in Arabic tweets                      3   \n","3    misogyny and sexism detection in Arabic tweets                      3   \n","4    misogyny and sexism detection in Arabic tweets                      3   \n","5    misogyny and sexism detection in Arabic tweets                      3   \n","..                                              ...                    ...   \n","141  misogyny and sexism detection in Arabic tweets                      3   \n","142  misogyny and sexism detection in Arabic tweets                      3   \n","143  misogyny and sexism detection in Arabic tweets                      3   \n","144  misogyny and sexism detection in Arabic tweets                      3   \n","145  misogyny and sexism detection in Arabic tweets                      3   \n","\n","    annotations      annotators lang  hard_label              soft_label  \\\n","1                Ann1,Ann2,Ann3   ar         NaN  {'0': None, '1': None}   \n","2                Ann1,Ann2,Ann3   ar         NaN  {'0': None, '1': None}   \n","3                Ann1,Ann2,Ann3   ar         NaN  {'0': None, '1': None}   \n","4                Ann1,Ann2,Ann3   ar         NaN  {'0': None, '1': None}   \n","5                Ann1,Ann2,Ann3   ar         NaN  {'0': None, '1': None}   \n","..          ...             ...  ...         ...                     ...   \n","141              Ann1,Ann2,Ann3   ar         NaN  {'0': None, '1': None}   \n","142              Ann1,Ann2,Ann3   ar         NaN  {'0': None, '1': None}   \n","143              Ann1,Ann2,Ann3   ar         NaN  {'0': None, '1': None}   \n","144              Ann1,Ann2,Ann3   ar         NaN  {'0': None, '1': None}   \n","145              Ann1,Ann2,Ann3   ar         NaN  {'0': None, '1': None}   \n","\n","    split                                         other_info  \n","1    test  {'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...  \n","2    test  {'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...  \n","3    test  {'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...  \n","4    test  {'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...  \n","5    test  {'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...  \n","..    ...                                                ...  \n","141  test  {'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...  \n","142  test  {'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...  \n","143  test  {'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...  \n","144  test  {'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...  \n","145  test  {'Ann1': 'Moderate_Female', 'Ann2': 'Liberal_F...  \n","\n","[145 rows x 10 columns]"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["df_test"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["145it [00:52,  2.78it/s]\n"]}],"source":["df_test['attention']=0\n","df_test['tokens']=''\n","for index, row in tqdm(df_test.iterrows()):\n","  attributions_ig, tokens = interpret_sentence(bert_model_wrapper, sentence=df_test.loc[index, 'text'], label=0)\n","  attributions_ig = attributions_ig.sum(dim=2).squeeze(0)\n","  attributions_ig = attributions_ig / torch.norm(attributions_ig)\n","  attributions_ig = attributions_ig.detach().cpu().data.numpy()\n","  df_test.loc[index, 'attention'] = str(attributions_ig)\n","  df_test.loc[index, 'tokens'] = str(tokens)"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["df_test.to_csv('./IG/arabic_test.csv', sep='\\t', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Semeval23","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"fa8861d37e63998a7b0ab8e344e18a68a9508aa82c22311e5e069af7f7720f68"}}},"nbformat":4,"nbformat_minor":0}
